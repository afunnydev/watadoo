# Management Backend

This project uses Golang for everything related to event management. This includes: event manager admin backend, crawlers, cron jobs, etc.

## Setup

This backend needs the local prisma setup done. Please see ```prisma``` for instructions.

1) Setup your environment variables

```
cp .env.example.local .env.development.local
nano .env.development.local

# in .env.development.local, use the same Google Maps API Key that you generated in the event-admin folder
GEOCODE_API_KEY="TheKeyGoesHere"
```

## GraphQL server

You can start a local GraphQL server with the following command:

```go run ./cmd/server```

It is a bit a pain on setup, because there's no way to register an admin user quickly. To make it simple, you can always disable the permission check on the queries and mutations you want to do.

This GraphQL server is generated by [gqlgen](https://gqlgen.com/). Please read their documentation before you continue.

### The Schema

This GraphQL server uses the schema defined in ```internal/resolvers/schema.graphql```. So, currently, if you modify the schema in the ```prisma``` folder, you need to manually make the changes in this folder also. I wrote a little go script for easier migration in another project, but haven't deployed in this one yet.

Once the schema is modified, there's some small steps needed to generate the new server:
1) If you added new types, enums or inputs, you need need to add them in the ```models``` section of the ```gqlgen.yml``` file. Use the path to the generated prisma file (should be easy to guess from other types).
2) Run ```go generate ./...``` to generate the server's files once again. 
3) If you added any Mutation or Query, you can grab the generated code in the ```tmp``` folder and copy it in the query.go or mutation.go file. You can then write your resolver.

It is a bit tedious, but we get a really fast and stable GraphQL Go Server :)

## Activate the crawlers

You can fetch all our sources of events with the following command:

```
go run ./cmd/automation
```

This will save all fetched events to ```output/events.json```, and not the the database. If you want to save the fetched events to the database, you can add the -save tag:

```
go run ./cmd/automation -save
```

### Spiders

I wrote a model for generic websites which looks like "list of events -> event". With this model, you can simply define a new Spider with the appropriate HTML selectors, URL, name, and you can fetch all the events in a breeze. This deserves a full section for understanding, which will come eventually.

### Facebook Crawler

You can easily fetch events for a FB page by supplying the URL for the events of a Page. You can all the URLs in ```cmd/automation/main.go``` for reference. When adding a new URL to the list, I strongly suggest having a look at the source first and check if it would cause a lot of duplicates. Be careful, some bad sources can mess things up! Also, before running the scraper, I strongly suggest to create all the venues manually based on the venues of the events for this page. Usually, that means creating 1-2 new venues. It will be easier for the crawler to associate the event with the correct venue.

### Custom Crawlers

Currently, the only custom crawler I wrote was for Tourisme Outaouais. Their site's HTML add pretty much nothing helpful in it, so I ended up hitting some endpoints directly for data. Anyway it can be taken for demonstration to understand how this could be done for more complicated sites.

### Left Undone

Currently, there's no automation related to managing multiple occurrences of an events. It is focused on fetching events without duplicates.

Also, the ```pkg/scraper``` is a real mess. I had not time to do some cleaning or write any docs, and I had no clear direction of where we were going.

Finally, there's no test written currently, which is definitely a pain. I would look into this quickly before making any change to understand how everything comes down together and make sure changes don't break anything. It's definitely not the most solid piece of software.

## Organize events

**COMMAND NOT COMPLETED**

This commands help you manage all events, their occurrences and their next occurrence date. Please refer to the file to understand every tasks that are happening. It runs nightly in a cron job on the live server. You can run the following command:

```
go run ./cmd/organize
```

Currently, all it does is to recalculate the next occurence date of an event, syncs the venues with WP and syncs the events with WP. Please note that if you want to sync with the real WP, the current username and password won't work (of course, lol).

What it should be doing is to manage recurring events better, delete old occurrences or old events and also spot duplicates (events and venues) for manual review. This is left to be implemented.
